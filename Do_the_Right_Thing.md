# Do the Right Thing

## Preface

It is to be noted that when any part of this paper appears dull there is a design in it. -Sir Richard Steele

My first glimmerings of interest in the topic of limited rationality appeared in 1983, during the first five minutes of the first lecture of my first AI class. "Intelligence," Professor Genesereth was saying, "consists of the successful carrying out of tasks normally considered to require intelligence - learning, reasoning, planning, communicating, problem-solving and so on." Not bad, as such introductions go. But something was missing. Should I consider a question-answering machine intelligent because it can answer questions by looking up the answers in some vast table? Or a theorem-proving machine intelligent because its blinding speed allows it to try all possible proofs of ever-increasing length? As befits an impoverished and not especially knowledgeable graduate student, it seemed to me that intelligence was intimately linked to the ability to succeed as far as possible given one's limited computational and informational resources. 

Fortunately, many researchers in many fields , including Michael Genesereth himself, have had similar intuitions over the past few decades, and the cumulative conceptual development is now poised to effect a revolution in the way we think about artificial intelligence, and in the way we build intelligent systems. This book , written by authors brought up in pre-revolutionary times, offers some intellectual justification for the coming change, provides a skeletal framework for the design of new systems, and describes some theoretical and practical tools for bringing about intelligent behavior in finite machines. I cannot yet offer a theory to replace the classical theories of perfect rationality , but it seems it is possible to go beyond merely pointing out their inadequacy as a theoretical foundation for AI。

The book assumes a basic acquaintance with fundamental concepts in artificial intelligence , including probability and logic , but despite this should be accessible to the informed layman, except perhaps for Chapters 4,5 and 6, which can be omitted at first reading ( and of course subsequent readings) if so desired. AI researchers of any school should find something to object to , and philosophers , economists and psychologists interested in rationality will have a field day. Computer scientists and system designers concerned with real-time systems may find some useful new ideas and tools , and chess players may find some new opposition.

My co-author, Eric Wefald, died tragically in August , 1989 with his wife Mary in a car accident while on holiday in France. I cannot begin to express the sense of loss felt by all who knew them. Eric was , in the words of Saul Kripke who advised him at Princeton , a rara avis indeed, equally at home translating Homer or proving theorems in meta-mathematics. From the PhD program in philosophy at Princeton , he went on to teach at New York University. His insatiable curiosity, together with the wish to end their geographical separation, led him to join his wife at Berkeley and to take up the field of artificial intelligence. NYU's loss was my great pain; I can safely say, as have his previous advisers , that I will never have a better student. Both the philosophical and the technical developments in this book are due in large part to Eric's brilliance. That most of the work was accomplished within two years of Eric's first exposure to computer science and artificial intelligence is a testament to this brilliance. Chapters 3 through 6 would have formed the major part of his dissertation, although there are many unfinished lines of thought, particularly in chapters 5 and 6, that I have been unable to complete. The latter two chapters were largely reconstructed from Eric's notes, annotations to programs and a talk given at the 1989 Machine Learning Workshop.

Stuart Russell

University of California,

Berkeley, 1991



## 1 Limited Rationality

*DAD: Son?*
*MOOKI: What Dad?*
*DAD: I've got some advice for you.*
*MOOKI: What's that Dad?*
*DAD: Do the right thing.*
*MOOKI: Do the right thing?*
*DAD: Yes.*
*MOOKI: That's it?*
*DAD: That's it.*
*MOOKI: OK.*
*-- Spike Lee, Do the Right Thing*

Intelligence and morality certainly seem to have something to do with doing the right thing. Economists, Philosophers and artificial intelligence researchers have tried, with some success, to make 'right' into a precise term. Unfortunately, inescapable constraints on the reasoning capabilities of any physical system make it impossible to do the right thing in all instances. A designer of intelligent systems therefore needs to forget about doing the right thing per se, and think instead about designing the right system, happy in the knowledge that this system must sometimes make mistakes. The right system should make as few mistakes as humanly or machinely possible. This is easier said than done, as we will discover, and that makes artificial intelligence interesting.

### 1.1 Introduction to Artificial Intelligence

Anyone teaching a course or writing a book with the same title as this section will have to decide what artificial intelligence is, even if only because inquiring minds want to know. Here are some representative 'opening lines' from various text and monographs:

* "Artificial Intelligence , the exciting new effort to make computers think ...... machines with minds, in the full and literal sense. " (Haugeland, 1985)
* "Artificial intelligence is the study of mental faculties through the use of computational models" (Charniak and McDermott , 1985)
* "Artificial Intelligence is the study of how to make computers do things at which , at the moment, people are better" (Rich, 1983)
* "Artificial intelligence is the study of intelligent behavior" (Genesereth and Nilsson, 1987)
* "AI is a field of study that seeks to explain and emulate intelligent behavior in terms of computational processes" (Schalkoff, 1990)

We see a full spectrum of definitions, from Haugeland's description of a field attempting to recreate human mental properties -- What Dennett has called the ‘intentional stance’ -- to a purely behavioral approach concerned only with performance, with (perhaps deftly) ambiguous statements from Charniak and McDermott and Rich in between. Although definitions in terms of intelligent behavior are not exceptionable, they don't provide much guidance. Rather than catalogue various types of intelligent behaviour, instead we can define AI as the problem of designing system that do the right thing. Provided a definition can be found for "right", this helps in two ways: first, it allows us to view such 'mental faculties' as planning and reasoning as occurring in the service of finding the right thing to do; second, it makes room for those among us (Brooks, 1986; Agre and Chapman, 1987) who take the position that systems can do the right thing without such mental faculties. (In fact, the multiplicity of ways to do the right thing is a central topic of this book)

Instead of assuming a certain collection of predefined and separately specified cognitive subsystems, one should first consider the intelligent entity as an agent, that is to say a system that senses its environment and acts upon it. This apparently retrograde step is crucial one, not least because it forces us to think more carefully about the specifications, boundaries and interconnections of subsystems. The 'whole agent' approach releases vision subsystems from the need to provide a fully labeled three-dimensional model of the scene, of use only to blind sculptors; relaxes the stricture that planning systems provide fully detailed and guaranteed plans, of use only to player -pianos; and prevents diagnosis systems from being designed to provide only the most probable disease, of use only to those who really do have the common cold and nothing more serious but less likely. The same step is taking place in the linguistics and philosophy communities. In large part, there fields have moved away from viewing sentences as expressions of propositions, and rationality as acquisition and transformation of true beliefs. Instead, utterances of sentences are viewed as 'speech acts' (Austin, 1962; Grice, 1975; Searle, 1969) carried out by a 'situated agent' (Barwise and Perry , 1983); and rationality is viewed as an adaptive equilibrium of perceptions , actions and motivations, rather than the creation of a faithful 'mirror of nature' (Rorty, 1979)

Without a definition for 'right' , however , Mooki is still in the dark. Here's where we get into trouble. Theoretical AI researchers have looked to classical, logical definitions of practical reasoning as finding a action that will achieve a specified goal , influenced , perhaps unintentionally , by McCarthy's (1958) early design for a program that could prove its way to the airport. Some have adopted the more flexible decision-theoretic definitions of rationality from economics, in which the right thing is that action which maximizes expected payoff (von Neumann, 1947). Armed with either these definitions , and with suitably encoded knowledge that allows them to be applied, the AI researcher has reduced the original problem of creating intelligence to what might appear to be merely a series of tricky implementation issues. Doyle's 'rational psychology' proposal (1983) argues persuasively that AI should ,and will, separate itself from cognitive psychology and computer science by concentrating on the core issue of the abstract design of rational systems, incorporating rational formation and transformation of beliefs, desires and intentions.

We discuss the exact shortcoming of the logical and decision-theoretic models in the next section. Briefly , all these approaches abstract away a crucial aspect of the process of doing the right thing, by associating 'rightness' with the action finally taken, rather than with the whole process of deliberating and acting. The difference is critical whenever mere finite machines attempt to calculate the recommendations of the theoretical models in non-trivial environments, since these recommendations can be arbitrarily had to determine. By focusing on the entire process, we can arrive at a problem statement for artificial intelligence that avoids the difficulties of the classical approaches. This and subsequent chapters discuss the problem statement , and use the intuitions it engenders to design architectures and algorithms more suited for machines of less than infinite speed. The keystone of our approach is the ability of reasoning systems to reason about their own deliberations , in order to make the best possible use of limited computational resources. Our results suggest that this ability is a useful one.

### 1.2 Agents, architectures and programs

The view of intelligent systems as agents focuses on the interaction between system and environment. An agent can be described abstractly as a mapping, not necessarily finite, from the sequence of sensory inputs (or 'percepts') up to the current time, to the action that the agent takes in response. That is , for each possible sequence of percepts, the mapping states which action the agent will take in response, as illustrated by figure 1.1. For example, a chess program can be described abstractly by stating which move it would make from any given position generated by a sequence of moves. The standard normative models of behavior , whether logical or decision -theoretic, should be regarded as specifying constraints on this mapping. These constraints say , roughly, that given the beliefs and goals ascribed to the agent as a result of its sequence of percepts, the selected action should be that which will achieve the goals according to the beliefs; or some equivalent expression in the language of probabilities and utilities. Thus a rational chess program , given the goal of winning, knowledge of the rules of chess, and a perception of the current position, should move in such a way as to guarantee a win if possible. This section and the next are about two ideas: first, that there is a important distinction to be kept in mind between the mapping and its implementation as a program running on some hardware; second, that is inappropriate to place rationality constraints on the mapping, since it is the behavior of the implementation that we care about.

There are, then three different ways to think about an agent. The first is the mapping from percepts sequences to actions; the second the program that implements it; and the third the behavior of the program as it runs. The relationship between programs and the mappings they implement is a fundamental one in computer science. A mapping is an entirely abstract entity; for example, if the percept sequences are of unbounded length then the mapping will be infinite, whereas the agent program is always finite. Computability theory is about which kinds of mappings can be represented by which kinds of program (or program/machine combinations); clearly, only some of the set of infinite mapping can be represented by a finite program , since there are may more infinite mappings than there are finite programs.

Failure to observe the distinction between the mapping and its implementation was one cause of the procedural-declarative  controversy of the 1970's, in which a main motivation for the declarative position was that a logical axiomatization of knowledge provided a clear specification of the mapping, whereas the procedural view was concerned more with its implementation in a program. Doyle(1983) has argued correctly that the positions are not inconsistent, since a procedural implementation is quite capable of representing a logically specified mapping. Many of the same issues have surfaced more recently in the debate between proponents of the deliberative and reactive approaches to action selection (Brooks, 1986; Agre and Chapman , 1987), the latter controversy being in many ways a reprise of the former. The declarative/deliberative position implements the mapping in the same way that the normative constraints are specified: explicit goal and belief structures are deliberated over by inference procedures in order to select actions. The procedural/reactive position is that all this deliberation is a waste of time -- why don't we just build agents that "do the right thing"? In this and subsequent sections we argue informally that there exist additional constraints arising from the nature of the design problem that make it far from trivial to just do it. In fact these constraints suggest that a hybrid representation of the mapping is needed, involving both declarative knowledge and direct tabulations.

We will start out with a pretty abstract specification of the design constraints , in order to avoid overcommitment that may result in inadequate systems; at the same time, it is important to understand the distinguishing characteristics of intelligence that constrain the choice of implementation, to render it more than an exercise in aesthetics or task-specific engineering. The agent design problem concerns both the architecture and its program:

* An architecture M is a fixed interpreter for the agent's program. In executing this program, the architecture extracts primitive percepts (for example, pixels or keystrokes) from the environment, running the program to obtain outputs (for example, motor commands or characters on a terminal). The architecture therefore also defines a programming language L for agent programs, namely the set of all programs the architecture will run.
* An agent program l ∈ L is a effective representation of perceptaction mapping. To be more precise, when it runs on the architecture it produces certain actions when the agent receives certain percept sequences. It is a continually running routine, rather than a simple 'one-shot' program that takes a percept sequence as input and generates an action. Formally, we can describe the mapping it represents as a function f: P* -> A where P is the set of percepts, P* is the set of all possible percept sequences, and A is the set of possible actions the agent can carry out in the external world.